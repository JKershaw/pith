# Benchmark Plan: 2025-12-30 (Comprehensive Run)

## Overview

This benchmark evaluates Pith's context quality against direct code exploration (Control agent) on **15 tasks** (3 per category) for statistical consistency.

## Repository

- **Repository**: Pith (self-test)
- **Size**: 24 TypeScript files, ~10.7k lines
- **Pith version**: afc708e (after PR #16 merge with refined LLM prompts)
- **Model**: qwen/qwen-turbo (per .env configuration)

## Selected Tasks

### Task Mix (per methodology)

| # | Type | Task | Rationale |
|---|------|------|-----------|
| 1 | Architecture | "What design patterns are used in this codebase?" | Tests pattern recognition across modules |
| 2 | Specific Behavior | "How does the buildPrompt function construct LLM prompts for different node types?" | Tests function-level detail capture |
| 3 | Cross-Module | "What are all the consumers of the WikiNode interface?" | Tests dependency tracing |
| 4 | Debugging | "API returns 404 for a file that exists. What could cause this?" | Tests error path understanding |
| 5 | Modification | "How would I add rate limiting to the API endpoints?" | Tests change impact analysis |

### Task Selection Criteria

- **Different from v3**: All 5 tasks are new (v3 tested: CLI orchestration, retry logic, AST→wiki flow, missing prose, JSDoc)
- **Coverage**: Architecture/patterns, function internals, type usage, error paths, API modification
- **Realistic**: Each represents a question a developer might actually ask

## Expected Duration

| Stage | Estimated Time |
|-------|----------------|
| Extraction | ~7-10s |
| Build | ~1-2s |
| Generation | ~120-200s |
| Task Evaluation (5 tasks × 2 approaches) | ~30-60 min |
| **Total** | ~1 hour |

## Cost Estimate

- Generation: ~$0.50-1.00 (qwen-turbo @ ~$0.50/M tokens)
- Control agent exploration: ~$1-2.00 (using Opus 4.5)
- Judge evaluation: ~$0.50-1.00
- **Total**: ~$2-4.00

## Methodology

1. Run Pith pipeline: `pith extract .` → `pith build` → `pith generate`
2. For each task:
   - Query Pith for relevant context
   - Spawn Control agent with Glob/Grep/Read access
   - Judge both contexts on 5 criteria (Relevance, Completeness, Accuracy, Efficiency, Actionability)
3. Compare results to v3 benchmark (Pith: 16.6/25, Control: 23.8/25)

## Success Criteria

- **Baseline**: Match or exceed v3 Pith average (16.6/25)
- **Improvement target**: Close the gap on Actionability (currently 2.0/5)
- **Information gap**: Identify specific missing information types
