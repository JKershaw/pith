# Benchmark Plan: 2025-12-30 Self-Test

## Repository

- **Target**: Pith (self-test)
- **Location**: `/home/user/pith`
- **Size**: 24 TypeScript files, ~10.5k lines
- **Commit**: c61a6e8

## Purpose

Follow-up benchmark to assess if any improvements have been made since the previous benchmark run (same date, earlier session). This run uses different task formulations to get additional coverage.

## Tasks Selected

| # | Category | Task |
|---|----------|------|
| 1 | Architecture | "Explain the three-stage pipeline (extract, build, generate) and data flow between stages" |
| 2 | Specific Behavior | "How does the AST extractor handle different TypeScript node types (functions, classes, interfaces)?" |
| 3 | Cross-module | "Trace how a WikiNode flows from creation in extractor to output in generator" |
| 4 | Debugging | "Generation fails with 'invalid model' error. What files and logic should I investigate?" |
| 5 | Modification | "I want to add Python support. What architectural changes are needed?" |

## Task Selection Rationale

- **Task 1**: Tests high-level architecture understanding (previous benchmark showed weakness here)
- **Task 2**: Tests function-level detail extraction (previous had only 1.8/5 completeness)
- **Task 3**: Tests relationship tracing across modules
- **Task 4**: Tests debugging with a new error scenario
- **Task 5**: Tests modification planning for a significant change

## Expected Duration

| Stage | Estimated Time |
|-------|---------------|
| Extraction | ~8s |
| Build | ~1s |
| Generation | ~2 min |
| Task Evaluation (5 tasks Ã— 2 agents) | ~10 min |
| **Total** | ~15 min |

## Estimated Cost

- Generation: ~$0.50-1.00 (qwen-turbo)
- Task evaluation: ~$0.20 (5 tasks, judge calls)
- **Total**: ~$1.00-1.50

## Comparison Baseline

Previous run (2025-12-30-self-test.md):
- Pith average: 12.6/25
- Control average: 24.2/25
- Win/Loss/Tie: 0-5-0

Goal: Assess if current state is similar and identify any improvements.
