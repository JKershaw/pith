# Benchmark Plan: 2026-01-03

## Overview

**Repository**: Pith (self-test)
**Type**: Query Mode benchmark
**Tasks**: All 15 tasks from the task bank

## Test Repository

| Property   | Value                                                    |
| ---------- | -------------------------------------------------------- |
| Repository | Pith (self-test)                                         |
| Size       | ~25k lines (estimated)                                   |
| Type       | TypeScript codebase                                      |
| Commit     | Current HEAD on `claude/setup-benchmarking-review-0uq1w` |

## Tasks to Evaluate

### Architecture Tasks (A1-A3)

- A1: "What are the main components of this codebase and how do they interact?"
- A2: "Explain the data flow from file input to wiki output."
- A3: "What design patterns are used in this codebase?"

### Specific Behavior Tasks (B1-B3)

- B1: "How does the extraction cache determine if a file needs re-extraction?"
- B2: "How does buildPrompt construct LLM prompts for different node types?"
- B3: "What is the retry logic in the LLM client and what triggers a retry?"

### Relationship Tasks (R1-R3)

- R1: "What files would be affected if I changed the WikiNode interface?"
- R2: "How do the API routes connect to the database layer?"
- R3: "What are all the consumers of the extractFile function?"

### Debugging Tasks (D1-D3)

- D1: "Generation completes but some nodes have empty prose. What should I investigate?"
- D2: "Why might the generate command be slow?"
- D3: "API returns 404 for a file that exists. What could cause this?"

### Modification Tasks (M1-M3)

- M1: "How would I add support for JavaScript (.js) files in addition to TypeScript?"
- M2: "How would I add rate limiting to the API endpoints?"
- M3: "I want to add a 'complexity' field to WikiNode. What files need changes?"

## Expected Duration

| Stage           | Estimated Time |
| --------------- | -------------- |
| Extraction      | ~20s           |
| Build           | ~10s           |
| Generation      | ~5 min         |
| Task Evaluation | ~30 min        |
| **Total**       | ~40-45 min     |

## Cost Estimate

- Model: qwen/qwen-turbo (default)
- Generation: ~$0.50-1.00
- Query calls: ~$0.20-0.50
- Control exploration: Minimal (uses Glob/Grep/Read)
- **Total estimated cost**: ~$1.00-1.50

## Evaluation Method

1. **Pith**: Use Query Mode (`POST /query`) for each task
2. **Control**: Spawn exploration agents with Glob, Grep, Read tools
3. **Scoring**: 4 criteria (Correctness, Completeness, Specificity, Conciseness) - 1-5 scale each
4. **Comparison**: Document win/loss/tie and calculate averages

## Success Criteria

- Complete all 15 tasks
- Record detailed scores and notes for each
- Compare with 2026-01-02 benchmark results
- Document any improvements or regressions
