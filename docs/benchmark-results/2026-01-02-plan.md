# Benchmark Plan: 2026-01-02

## Objective

Run a comprehensive self-test benchmark on the Pith codebase using **Query Mode** to evaluate the end-to-end question-answering capability.

## Repository

- **Name**: Pith (self-test)
- **Location**: /home/user/pith
- **Size**: ~25k lines TypeScript
- **Version**: 973c489

## Tasks

All 15 tasks from the task bank (Appendix A of BENCHMARKING.md):

### Architecture Tasks (A1-A3)

- A1: "What are the main components of this codebase and how do they interact?"
- A2: "Explain the data flow from file input to wiki output."
- A3: "What design patterns are used in this codebase?"

### Specific Behavior Tasks (B1-B3)

- B1: "How does the extraction cache determine if a file needs re-extraction?"
- B2: "How does buildPrompt construct LLM prompts for different node types?"
- B3: "What is the retry logic in the LLM client and what triggers a retry?"

### Relationship Tasks (R1-R3)

- R1: "What files would be affected if I changed the WikiNode interface?"
- R2: "How do the API routes connect to the database layer?"
- R3: "What are all the consumers of the extractFile function?"

### Debugging Tasks (D1-D3)

- D1: "Generation completes but some nodes have empty prose. What should I investigate?"
- D2: "Why might the generate command be slow?"
- D3: "API returns 404 for a file that exists. What could cause this?"

### Modification Tasks (M1-M3)

- M1: "How would I add support for JavaScript (.js) files in addition to TypeScript?"
- M2: "How would I add rate limiting to the API endpoints?"
- M3: "I want to add a 'complexity' field to WikiNode. What files need changes?"

## Evaluation Mode

**Query Mode** (`POST /query`) - Tests end-to-end capability including:

- Automatic file selection via keyword matching
- Answer synthesis from selected file context
- Line number references in answers

## Expected Duration

| Stage                                     | Estimated Time |
| ----------------------------------------- | -------------- |
| Extraction                                | ~15-20s        |
| Build                                     | ~5s            |
| Generation                                | ~4-5 min       |
| Task evaluation (15 tasks x 2 approaches) | ~30-45 min     |
| **Total**                                 | ~40-50 min     |

## Estimated Cost

- Generation phase: ~$0.50-1.00 (qwen-turbo)
- Query evaluations: ~$0.20-0.30 (15 queries)
- Control agent exploration: ~$1.00-2.00 (Claude subagent time)
- **Total**: ~$2-3

## Methodology

For each task:

1. **Pith Query**: `POST /query` with task question, record answer/files/timing
2. **Control Agent**: Subagent with Glob/Grep/Read tools explores codebase
3. **Judge**: Score both on Correctness, Completeness, Specificity, Conciseness (1-5 each)

## Success Criteria

- Complete all 15 tasks
- Compare to 2026-01-01 Query Mode baseline (Pith: 73.5%, Control: 95%)
- Identify any regressions or improvements

## Comparison Benchmarks

| Date       | Mode    | Pith Score | Control Score | Notes                      |
| ---------- | ------- | ---------- | ------------- | -------------------------- |
| 2026-01-01 | Query   | 73.5%      | 95.0%         | First Query Mode benchmark |
| 2026-01-01 | Context | 72.8%      | 96.4%         | Context Mode baseline      |
